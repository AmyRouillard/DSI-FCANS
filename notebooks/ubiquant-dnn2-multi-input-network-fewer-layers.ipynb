{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import Packages","metadata":{}},{"cell_type":"code","source":"import os ## fxns for interacting with the OS\nimport pandas as pd ## data manipulation\nimport numpy as np ## mathematical fxns\nimport gc ## automatically releases memory when an object is no longer used\nimport matplotlib.pyplot as plt ## plotting\nimport tensorflow as tf ## deep learning\nfrom tensorflow.keras import layers ## deep learning\nfrom tensorflow import keras ## deep learning\nfrom scipy import stats ## scientific computing and technical computing","metadata":{"execution":{"iopub.status.busy":"2022-02-26T10:30:22.46843Z","iopub.execute_input":"2022-02-26T10:30:22.468775Z","iopub.status.idle":"2022-02-26T10:30:27.763474Z","shell.execute_reply.started":"2022-02-26T10:30:22.468668Z","shell.execute_reply":"2022-02-26T10:30:27.762734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Remove scientific notation\npd.set_option('display.float_format', lambda x: '%.2f' % x)","metadata":{"execution":{"iopub.status.busy":"2022-02-26T10:30:47.857469Z","iopub.execute_input":"2022-02-26T10:30:47.858271Z","iopub.status.idle":"2022-02-26T10:30:47.863259Z","shell.execute_reply.started":"2022-02-26T10:30:47.858223Z","shell.execute_reply":"2022-02-26T10:30:47.862485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import Dataset","metadata":{}},{"cell_type":"markdown","source":"This competition's dataset (18.55gb) is too large. We will use another dataset converted to utilize less memory in pickle format.","metadata":{}},{"cell_type":"code","source":"%%time\nn_features = 300\nfeatures = [f'f_{i}' for i in range(n_features)]\ntrain = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-26T10:30:51.053427Z","iopub.execute_input":"2022-02-26T10:30:51.05431Z","iopub.status.idle":"2022-02-26T10:31:09.391663Z","shell.execute_reply.started":"2022-02-26T10:30:51.054259Z","shell.execute_reply":"2022-02-26T10:31:09.390978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Investment ID","metadata":{}},{"cell_type":"code","source":"## Select Investment Id from dataframe\n\ninvestment_id = train.pop(\"investment_id\")\n\nprint(\"Unique Investment IDs : {}\".format(investment_id.nunique()))\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-26T10:31:09.394286Z","iopub.execute_input":"2022-02-26T10:31:09.394544Z","iopub.status.idle":"2022-02-26T10:31:09.429918Z","shell.execute_reply.started":"2022-02-26T10:31:09.394509Z","shell.execute_reply":"2022-02-26T10:31:09.429238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(investment_id.head())\nprint(investment_id.describe())","metadata":{"execution":{"iopub.status.busy":"2022-02-26T10:31:11.92733Z","iopub.execute_input":"2022-02-26T10:31:11.927624Z","iopub.status.idle":"2022-02-26T10:31:11.989872Z","shell.execute_reply.started":"2022-02-26T10:31:11.927593Z","shell.execute_reply":"2022-02-26T10:31:11.988974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Drop Time Id from dataframe\n_ = train.pop(\"time_id\")","metadata":{"execution":{"iopub.status.busy":"2022-02-26T10:32:12.534643Z","iopub.execute_input":"2022-02-26T10:32:12.535357Z","iopub.status.idle":"2022-02-26T10:32:12.546343Z","shell.execute_reply.started":"2022-02-26T10:32:12.535316Z","shell.execute_reply":"2022-02-26T10:32:12.545681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Select Target feature from dataframe\ny = train.pop(\"target\")\ny.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-26T10:32:23.167115Z","iopub.execute_input":"2022-02-26T10:32:23.167376Z","iopub.status.idle":"2022-02-26T10:32:23.180056Z","shell.execute_reply.started":"2022-02-26T10:32:23.167346Z","shell.execute_reply":"2022-02-26T10:32:23.179292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create a IntegerLookup layer for investment_id input\n","metadata":{}},{"cell_type":"markdown","source":"A integer lookup layer is a preprocessing layer which maps integer features to contiguous ranges. Turns integer categorical values into an encoded representation that can be read by an Embedding layer or Dense layer.\n\nThe integer lookup layer will be one of the two input branches for the multi-input keras model.","metadata":{}},{"cell_type":"code","source":"%%time\ninvestment_ids = list(investment_id.unique())\ninvestment_id_size = len(investment_ids) + 1\ninvestment_id_lookup_layer = layers.IntegerLookup(max_tokens=investment_id_size)\ninvestment_id_lookup_layer.adapt(pd.DataFrame({\"investment_ids\":investment_ids}))","metadata":{"execution":{"iopub.status.busy":"2022-02-26T10:32:28.697074Z","iopub.execute_input":"2022-02-26T10:32:28.697722Z","iopub.status.idle":"2022-02-26T10:32:31.51314Z","shell.execute_reply.started":"2022-02-26T10:32:28.697645Z","shell.execute_reply":"2022-02-26T10:32:31.512106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make Tensorflow dataset","metadata":{}},{"cell_type":"markdown","source":"Define functions to create a dataset from input features and preprocess the input data.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\ndef preprocess(X, y):\n    return X, y\ndef make_dataset(feature, investment_id, y, batch_size=1024, mode=\"train\"):\n    \n    \"\"\"\n    \n    1. Create a source dataset from your input data.\n    2. Apply dataset transformations to preprocess the data.\n    3. Iterate over the dataset and process the elements.\n    \n    \"\"\"\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature), y)) ## read elements from memory\n    ds = ds.map(preprocess)\n    if mode == \"train\":\n        ds = ds.shuffle(4096)\n    \n    ## Combine consecutive elements of this dataset into batches.\n    ## Cache the elements in dataset\n    ## allow later elements to be prepared while the current element is being processed (prefetch)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE) \n    return ds","metadata":{"execution":{"iopub.status.busy":"2022-02-26T10:32:39.122448Z","iopub.execute_input":"2022-02-26T10:32:39.123224Z","iopub.status.idle":"2022-02-26T10:32:39.129923Z","shell.execute_reply.started":"2022-02-26T10:32:39.123185Z","shell.execute_reply":"2022-02-26T10:32:39.129111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling","metadata":{}},{"cell_type":"markdown","source":"The model architecture is a multi input keras network with 2 input branches. First branch handles investment Ids while the second branch will handle remaining anonymalized 300 features.","metadata":{}},{"cell_type":"code","source":"def get_model():\n    \n    \"\"\"\n    Fxn to define model architecture: Multi input keras model\n    \"\"\"\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x) ## Turns positive integers (indexes) into dense vectors of fixed size\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    \n    ## Takes as input a list of tensors and returns a single tensor that is the concatenation of all inputs\n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    \n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    \n    output = layers.Dense(1)(x)\n    \n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    \n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    \n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-26T10:32:44.893816Z","iopub.execute_input":"2022-02-26T10:32:44.894111Z","iopub.status.idle":"2022-02-26T10:32:44.906755Z","shell.execute_reply.started":"2022-02-26T10:32:44.894078Z","shell.execute_reply":"2022-02-26T10:32:44.905942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Model summary and visualizating layout\n\nmodel = get_model()\nmodel.summary()\nkeras.utils.plot_model(model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-26T10:32:47.050109Z","iopub.execute_input":"2022-02-26T10:32:47.050393Z","iopub.status.idle":"2022-02-26T10:32:48.12217Z","shell.execute_reply.started":"2022-02-26T10:32:47.05036Z","shell.execute_reply":"2022-02-26T10:32:48.121375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cross Validation ","metadata":{}},{"cell_type":"code","source":"%%time\n## Stratified is to ensure that each fold of dataset has the same proportion of observations with a given label.\nfrom sklearn.model_selection import StratifiedKFold\n## Create 5 folds\nkfold = StratifiedKFold(5, shuffle=True, random_state=42)\nmodels = []\nfor index, (train_indices, valid_indices) in enumerate(kfold.split(train, investment_id)):\n    ## Split dataset\n    X_train, X_val = train.iloc[train_indices], train.iloc[valid_indices]\n    investment_id_train = investment_id[train_indices]\n    y_train, y_val = y.iloc[train_indices], y.iloc[valid_indices]\n    investment_id_val = investment_id[valid_indices]\n    train_ds = make_dataset(X_train, investment_id_train, y_train)\n    valid_ds = make_dataset(X_val, investment_id_val, y_val, mode=\"valid\")\n    \n    ## Call model\n    model = get_model()\n    \n    ## Use callbacks to stop model training if model perfomance is not improving\n    checkpoint = keras.callbacks.ModelCheckpoint(f\"model_{index}\", save_best_only=True)\n    early_stop = keras.callbacks.EarlyStopping(patience=10)\n    \n    ## Fit model\n    history = model.fit(train_ds, epochs=30, validation_data=valid_ds, callbacks=[checkpoint, early_stop])\n    \n    ## append model to models list\n    models.append(keras.models.load_model(f\"model_{index}\"))\n    \n    ## Make predictions for validation set and get pearson correlation coefficient\n    pearson_score = stats.pearsonr(model.predict(valid_ds).ravel(), y_val.values)[0]\n    print('Pearson:', pearson_score)\n    \n    ## Create a dataframe of mean squared errors for train and validation sets and plot the metrics\n    pd.DataFrame(history.history, columns=[\"mse\", \"val_mse\"]).plot()\n    plt.title(\"MSE\")\n    plt.show()\n    \n    ## Create a dataframe of mean absolute errors for train and validation sets and plot the metrics\n    pd.DataFrame(history.history, columns=[\"mae\", \"val_mae\"]).plot()\n    plt.title(\"MAE\")\n    plt.show()\n    \n    ## Create a dataframe of root mean squared errors for train and validation sets and plot the metrics\n    pd.DataFrame(history.history, columns=[\"rmse\", \"val_rmse\"]).plot()\n    plt.title(\"RMSE\")\n    plt.show()\n    \n    ## Remove un-neccesary objects from memory\n    del investment_id_train\n    del investment_id_val\n    del X_train\n    del X_val\n    del y_train\n    del y_val\n    del train_ds\n    del valid_ds\n    gc.collect()\n    break","metadata":{"execution":{"iopub.status.busy":"2022-02-26T10:32:54.408933Z","iopub.execute_input":"2022-02-26T10:32:54.409219Z","iopub.status.idle":"2022-02-26T10:37:53.789486Z","shell.execute_reply.started":"2022-02-26T10:32:54.409186Z","shell.execute_reply":"2022-02-26T10:37:53.788732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"## Functions to pre-process test set\n\ndef preprocess_test(investment_id, feature):\n    return (investment_id, feature), 0\n\ndef make_test_dataset(feature, investment_id, batch_size=1024):\n    \"\"\"\n    Make test dataset from test features and preprocess test set\n    \"\"\"\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature)))\n    ds = ds.map(preprocess_test)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\n\ndef inference(models, ds):\n    \"\"\"\n    Make predictions from the models and return mean of predictions\n    \n    \"\"\"\n    y_preds = []\n    for model in models:\n        y_pred = model.predict(ds)\n        y_preds.append(y_pred)\n    return np.mean(y_preds, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T13:46:33.165549Z","iopub.execute_input":"2022-02-25T13:46:33.166614Z","iopub.status.idle":"2022-02-25T13:46:33.175405Z","shell.execute_reply.started":"2022-02-25T13:46:33.166559Z","shell.execute_reply":"2022-02-25T13:46:33.174305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Call Kaggle API to make predictions\nimport ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test() \nfor (test_df, sample_prediction_df) in iter_test:\n    ds = make_test_dataset(test_df[features], test_df[\"investment_id\"])\n    sample_prediction_df['target'] = inference(models, ds)\n    env.predict(sample_prediction_df)","metadata":{},"execution_count":null,"outputs":[]}]}