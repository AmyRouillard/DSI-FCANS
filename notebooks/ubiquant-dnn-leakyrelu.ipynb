{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import datasets to workspace\n\n1. Competition dataset. \n2. Memory optimized parquet dataset.\n\nThe competition dataset is large (18.55 GB) we will use a more memory optimized dataset (3.63 GB) in parquet format. Credit to @Rob Mulla for sharing the optimized dataset.","metadata":{}},{"cell_type":"code","source":"## Import Libraries\nimport pandas as pd\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.preprocessing import StandardScaler\nimport ubiquant\nfrom sklearn.model_selection import KFold\nfrom scipy.stats import pearsonr","metadata":{"execution":{"iopub.status.busy":"2022-02-26T12:22:23.859168Z","iopub.execute_input":"2022-02-26T12:22:23.859865Z","iopub.status.idle":"2022-02-26T12:22:29.777101Z","shell.execute_reply.started":"2022-02-26T12:22:23.859748Z","shell.execute_reply":"2022-02-26T12:22:29.775955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Remove scientific notation\npd.set_option('display.float_format', lambda x: '%.2f' % x)","metadata":{"execution":{"iopub.status.busy":"2022-02-26T12:22:29.779425Z","iopub.execute_input":"2022-02-26T12:22:29.779783Z","iopub.status.idle":"2022-02-26T12:22:29.784699Z","shell.execute_reply.started":"2022-02-26T12:22:29.77974Z","shell.execute_reply":"2022-02-26T12:22:29.783562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Explore working environment\nos.listdir('../input/ubiquant-market-prediction')\n","metadata":{"execution":{"iopub.status.busy":"2022-02-26T12:22:49.094191Z","iopub.execute_input":"2022-02-26T12:22:49.094527Z","iopub.status.idle":"2022-02-26T12:22:49.103696Z","shell.execute_reply.started":"2022-02-26T12:22:49.094481Z","shell.execute_reply":"2022-02-26T12:22:49.103045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n## Import Dataset\ndf = pd.read_parquet('../input/ubiquant-parquet/train_low_mem.parquet')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-26T12:23:06.988018Z","iopub.execute_input":"2022-02-26T12:23:06.988331Z","iopub.status.idle":"2022-02-26T12:23:48.379265Z","shell.execute_reply.started":"2022-02-26T12:23:06.988304Z","shell.execute_reply":"2022-02-26T12:23:48.378326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Dataframe shape and size\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2022-02-26T12:24:13.172728Z","iopub.execute_input":"2022-02-26T12:24:13.173088Z","iopub.status.idle":"2022-02-26T12:24:13.205602Z","shell.execute_reply.started":"2022-02-26T12:24:13.173049Z","shell.execute_reply":"2022-02-26T12:24:13.20491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Features\ndf.columns","metadata":{"execution":{"iopub.status.busy":"2022-02-26T12:24:17.856234Z","iopub.execute_input":"2022-02-26T12:24:17.856604Z","iopub.status.idle":"2022-02-26T12:24:17.864648Z","shell.execute_reply.started":"2022-02-26T12:24:17.856564Z","shell.execute_reply":"2022-02-26T12:24:17.863858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Unique rows\ndf.row_id.nunique()","metadata":{"execution":{"iopub.status.busy":"2022-02-26T12:24:39.03131Z","iopub.execute_input":"2022-02-26T12:24:39.032215Z","iopub.status.idle":"2022-02-26T12:24:40.985155Z","shell.execute_reply.started":"2022-02-26T12:24:39.032175Z","shell.execute_reply":"2022-02-26T12:24:40.984264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Missing Values \ndf.isnull().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-26T12:24:45.422051Z","iopub.execute_input":"2022-02-26T12:24:45.422424Z","iopub.status.idle":"2022-02-26T12:24:47.711055Z","shell.execute_reply.started":"2022-02-26T12:24:45.422388Z","shell.execute_reply":"2022-02-26T12:24:47.710121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Select feature columns\nf_col = df.drop(['row_id','time_id','investment_id','target'],axis=1).columns\nprint(len(f_col))\nf_col","metadata":{"execution":{"iopub.status.busy":"2022-02-26T12:25:11.901845Z","iopub.execute_input":"2022-02-26T12:25:11.903599Z","iopub.status.idle":"2022-02-26T12:25:17.287147Z","shell.execute_reply.started":"2022-02-26T12:25:11.903521Z","shell.execute_reply":"2022-02-26T12:25:17.286256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Investment ID","metadata":{}},{"cell_type":"markdown","source":"Since \"f_#\" columns are similar to standard normal distribution, \"investment_id\" column will be converted for efficient training.\n\nTo apply the same criteria at test dataset, make the scaler and use it later.","metadata":{}},{"cell_type":"code","source":"## Scale investment id\nprint(df['investment_id'].describe())","metadata":{"execution":{"iopub.status.busy":"2022-02-26T12:27:04.520755Z","iopub.execute_input":"2022-02-26T12:27:04.52113Z","iopub.status.idle":"2022-02-26T12:27:04.607706Z","shell.execute_reply.started":"2022-02-26T12:27:04.521097Z","shell.execute_reply":"2022-02-26T12:27:04.606738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['investment_id'].value_counts().describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-26T12:27:08.425988Z","iopub.execute_input":"2022-02-26T12:27:08.426313Z","iopub.status.idle":"2022-02-26T12:27:08.453983Z","shell.execute_reply.started":"2022-02-26T12:27:08.426283Z","shell.execute_reply":"2022-02-26T12:27:08.453134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## StandardScaler standardizes a feature by subtracting the mean and then scaling to unit variance. \n## Unit variance means dividing all the values by the standard deviation.\nscaler = StandardScaler()\nscaler.fit(pd.DataFrame(df['investment_id']))","metadata":{"execution":{"iopub.status.busy":"2022-02-26T12:27:16.418003Z","iopub.execute_input":"2022-02-26T12:27:16.41833Z","iopub.status.idle":"2022-02-26T12:27:16.515285Z","shell.execute_reply.started":"2022-02-26T12:27:16.418298Z","shell.execute_reply":"2022-02-26T12:27:16.514328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Pre-processing Function","metadata":{}},{"cell_type":"code","source":"def make_dataset(df):\n    \n    \"\"\"\n    \n    Fxn to:\n    1. Scale investment ID\n    2. Concatenate scaled Investment IDs with anonymized features\n    \n    \"\"\"\n    inv_df = df['investment_id']\n    f_df = df[f_col]\n    scaled_investment_id = scaler.transform(pd.DataFrame(inv_df))\n    df['investment_id'] = scaled_investment_id\n    data_x = pd.concat([df['investment_id'], f_df], axis=1)\n    return data_x","metadata":{"execution":{"iopub.status.busy":"2022-02-26T12:30:47.446179Z","iopub.execute_input":"2022-02-26T12:30:47.446547Z","iopub.status.idle":"2022-02-26T12:30:47.454452Z","shell.execute_reply.started":"2022-02-26T12:30:47.446508Z","shell.execute_reply":"2022-02-26T12:30:47.453338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Change the data type\nNotebook memory has limit which is too small to use raw data.\n\nSo, change the data type to \"float16\".\n\nAnd divide the dataset into variables for input and output.","metadata":{}},{"cell_type":"code","source":"## Convert data types and pre process\ndf=df.astype('float16')\ndf_x = make_dataset(df)\ndf_x","metadata":{"execution":{"iopub.status.busy":"2022-02-26T12:30:51.83317Z","iopub.execute_input":"2022-02-26T12:30:51.833475Z","iopub.status.idle":"2022-02-26T12:31:04.912657Z","shell.execute_reply.started":"2022-02-26T12:30:51.833439Z","shell.execute_reply":"2022-02-26T12:31:04.911384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Glimpse of new scaled investment ID\ndf_x['investment_id'].describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-26T12:33:47.850945Z","iopub.execute_input":"2022-02-26T12:33:47.851282Z","iopub.status.idle":"2022-02-26T12:33:48.343335Z","shell.execute_reply.started":"2022-02-26T12:33:47.851252Z","shell.execute_reply":"2022-02-26T12:33:48.342401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Target variable\ndf_y = pd.DataFrame(df['target'])\ndf_y","metadata":{"execution":{"iopub.status.busy":"2022-02-26T12:34:11.297737Z","iopub.execute_input":"2022-02-26T12:34:11.298666Z","iopub.status.idle":"2022-02-26T12:34:11.314122Z","shell.execute_reply.started":"2022-02-26T12:34:11.29861Z","shell.execute_reply":"2022-02-26T12:34:11.313379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Target feature distribution\ndf_y.describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-26T12:34:17.499118Z","iopub.execute_input":"2022-02-26T12:34:17.499613Z","iopub.status.idle":"2022-02-26T12:34:18.138768Z","shell.execute_reply.started":"2022-02-26T12:34:17.499581Z","shell.execute_reply":"2022-02-26T12:34:18.137951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Target feature plot\ndf_y.plot()","metadata":{"execution":{"iopub.status.busy":"2022-02-26T12:34:21.913155Z","iopub.execute_input":"2022-02-26T12:34:21.91354Z","iopub.status.idle":"2022-02-26T12:34:24.566464Z","shell.execute_reply.started":"2022-02-26T12:34:21.913499Z","shell.execute_reply":"2022-02-26T12:34:24.565809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Delete raw data\ndel df","metadata":{"execution":{"iopub.status.busy":"2022-02-26T12:34:42.090297Z","iopub.execute_input":"2022-02-26T12:34:42.091501Z","iopub.status.idle":"2022-02-26T12:34:42.100816Z","shell.execute_reply.started":"2022-02-26T12:34:42.09141Z","shell.execute_reply":"2022-02-26T12:34:42.099958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling","metadata":{}},{"cell_type":"markdown","source":"We will use a simple deep neural network.\n\nThe brief descriptions are as follows:\n\n1. Use LeakyReLU activation.\nimproved version of the ReLU activation function.\n\n2. Use BatchNormalization.\nmethod used to make artificial neural networks faster and more stable through normalization of the layers' inputs by re-centering and re-scaling. It is mainly used before activation function layer.\n\n3. Use Dropout.\nTo prevent overfitting.\n\n4. Use kernel_initializer with 'he_normal'.\nInitializers define the way to set the initial random weights of Keras layers\n'he_normal' initializer strategy works well with derivatives of relu.\nIt draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / fan_in) where fan_in is the number of input units in the weight tensor.\n\n5. Use ExponentialDecay scheduling.\nWhen training a model, it is often useful to lower the learning rate as the training progresses. This schedule applies an exponential decay function to an optimizer step, given a provided initial learning rate.It will be great for improving your performance.\n\n6. Use ModelCheckpoint.\nTo save model best performance, we will use ModelChechpoint in callbacks parameter.","metadata":{}},{"cell_type":"code","source":"def tm_model():\n    \n    \"\"\"\n    Model architecture defination\n    \"\"\"\n    inputs_ = tf.keras.Input(shape = [df_x.shape[1]])\n    \n    x = tf.keras.layers.Dense(64, kernel_initializer = 'he_normal')(inputs_)\n    batch = tf.keras.layers.BatchNormalization()(x)\n    leaky = tf.keras.layers.LeakyReLU(0.1)(batch)\n    \n    x = tf.keras.layers.Dense(128, kernel_initializer = 'he_normal')(leaky)\n    batch = tf.keras.layers.BatchNormalization()(x)\n    leaky = tf.keras.layers.LeakyReLU(0.1)(batch)\n    \n    x = tf.keras.layers.Dense(256, kernel_initializer = 'he_normal')(leaky)\n    batch = tf.keras.layers.BatchNormalization()(x)\n    leaky = tf.keras.layers.LeakyReLU(0.1)(batch)\n    \n    x = tf.keras.layers.Dense(512, kernel_initializer = 'he_normal')(leaky)\n    batch = tf.keras.layers.BatchNormalization()(x)\n    leaky = tf.keras.layers.LeakyReLU(0.1)(batch)\n    \n    x = tf.keras.layers.Dense(256, kernel_initializer = 'he_normal')(leaky)\n    batch = tf.keras.layers.BatchNormalization()(x)\n    leaky = tf.keras.layers.LeakyReLU(0.1)(batch)\n    drop = tf.keras.layers.Dropout(0.4)(leaky)\n    \n    x = tf.keras.layers.Dense(128, kernel_initializer = 'he_normal')(drop)\n    batch = tf.keras.layers.BatchNormalization()(x)\n    leaky = tf.keras.layers.LeakyReLU(0.1)(batch)\n    \n    x = tf.keras.layers.Dense(8, kernel_initializer = 'he_normal')(leaky)\n    batch = tf.keras.layers.BatchNormalization()(x)\n    leaky = tf.keras.layers.LeakyReLU(0.1)(batch)\n    drop = tf.keras.layers.Dropout(0.4)(leaky)\n    \n    outputs_ = tf.keras.layers.Dense(1)(drop)\n    \n    model = tf.keras.Model(inputs = inputs_, outputs = outputs_)\n    \n    rmse = tf.keras.metrics.RootMeanSquaredError()\n\n    learning_sch = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate = 0.003,\n    decay_steps = 9700,\n    decay_rate = 0.98)\n    adam = tf.keras.optimizers.Adam(learning_rate = learning_sch)\n    \n    model.compile(loss = 'mse', metrics = rmse, optimizer = adam)\n    return model\n\ntm_model().summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-26T12:49:11.612669Z","iopub.execute_input":"2022-02-26T12:49:11.613136Z","iopub.status.idle":"2022-02-26T12:49:13.158191Z","shell.execute_reply.started":"2022-02-26T12:49:11.613105Z","shell.execute_reply":"2022-02-26T12:49:13.157309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Plot Model\ntf.keras.utils.plot_model(tm_model(),show_shapes=True,expand_nested=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T12:51:50.093303Z","iopub.execute_input":"2022-02-22T12:51:50.093609Z","iopub.status.idle":"2022-02-22T12:51:51.926706Z","shell.execute_reply.started":"2022-02-22T12:51:50.093577Z","shell.execute_reply":"2022-02-22T12:51:51.925436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## KFold Strategy","metadata":{}},{"cell_type":"code","source":"## Split data into 5 folds with shuffling\nkfold_generator = KFold(n_splits =5, shuffle=True, random_state = 2022)\nkfold_generator","metadata":{"execution":{"iopub.status.busy":"2022-02-26T12:58:31.682176Z","iopub.execute_input":"2022-02-26T12:58:31.682504Z","iopub.status.idle":"2022-02-26T12:58:31.689682Z","shell.execute_reply.started":"2022-02-26T12:58:31.682453Z","shell.execute_reply":"2022-02-26T12:58:31.689057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Fitting","metadata":{}},{"cell_type":"code","source":"%%time\ncallbacks = tf.keras.callbacks.ModelCheckpoint('tm_model.h5', save_best_only = True)\nfor train_index, val_index in kfold_generator.split(df_x, df_y):\n    # Split training dataset.\n    train_x, train_y = df_x.iloc[train_index], df_y.iloc[train_index]\n    # Split validation dataset.\n    val_x, val_y = df_x.iloc[val_index], df_y.iloc[val_index]\n    # Make dataset.\n    tf_train = tf.data.Dataset.from_tensor_slices((train_x, train_y)).shuffle(2022).batch(1024, drop_remainder=True).prefetch(1)\n    tf_val = tf.data.Dataset.from_tensor_slices((val_x, val_y)).shuffle(2022).batch(1024, drop_remainder=True).prefetch(1)\n    \n    # Load model\n    model = tm_model()\n    \n    # Model fitting\n    \n    ## Initial run with 5 epochs (epochs should be increased)\n    \n    model.fit(tf_train, callbacks = callbacks, epochs = 20, #### change the epochs into more numbers.\n             validation_data = (tf_val), shuffle=True)\n    # Delete tensor dataset and model for avoiding memory exploring.\n    del tf_train\n    del tf_val\n    del model","metadata":{"execution":{"iopub.status.busy":"2022-02-26T12:59:59.773826Z","iopub.execute_input":"2022-02-26T12:59:59.774206Z","iopub.status.idle":"2022-02-26T13:00:45.191827Z","shell.execute_reply.started":"2022-02-26T12:59:59.774168Z","shell.execute_reply":"2022-02-26T13:00:45.189551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model = tf.keras.models.load_model('tm_model.h5')\nenv = ubiquant.make_env()   \niter_test = env.iter_test()    \nfor (test_df, sample_prediction_df) in iter_test:\n    test_df = make_dataset(test_df)\n    sample_prediction_df['target'] = best_model.predict(test_df)  \n    env.predict(sample_prediction_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T14:06:13.305943Z","iopub.execute_input":"2022-02-22T14:06:13.306428Z","iopub.status.idle":"2022-02-22T14:06:14.361748Z","shell.execute_reply.started":"2022-02-22T14:06:13.306392Z","shell.execute_reply":"2022-02-22T14:06:14.360795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}