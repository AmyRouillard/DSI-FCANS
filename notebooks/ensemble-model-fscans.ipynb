{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nThis notebook details the ensemble model consisting of three separate  deep neural network models. The model weights are loaded and averaged in this notebook. Details and code for the separate models can be found here:\n1. [Model 1](https://github.com/AmyRouillard/DSI-FCANS/blob/development/notebooks/dnn-base-model-1.ipynb)\n2. [Model 2](https://github.com/AmyRouillard/DSI-FCANS/blob/development/notebooks/model-2-10fold-model-2.ipynb)\n3. [Model 3](https://github.com/AmyRouillard/DSI-FCANS/blob/development/notebooks/model-3-10fold.ipynb)\n\nThis notebook is made up of the following sections:\n\n1. Importing libraries\n2. Data importation\n3. Data wrangling\n4. Utility functions\n5. Model architecture and Load Weights\n6. Model Ensemble\n7. Api Submission\n","metadata":{}},{"cell_type":"markdown","source":"## 1. Importing Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow import keras","metadata":{"execution":{"iopub.status.busy":"2022-03-06T09:41:24.853154Z","iopub.execute_input":"2022-03-06T09:41:24.853663Z","iopub.status.idle":"2022-03-06T09:41:24.859358Z","shell.execute_reply.started":"2022-03-06T09:41:24.853625Z","shell.execute_reply":"2022-03-06T09:41:24.858685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Data Importation","metadata":{}},{"cell_type":"code","source":"# Track time to load dataset\n!%%time\n\n# Declare number of ananonymized features\nn_features = 300\n\n# Select anonymized features\nfeatures = [f'f_{i}' for i in range(n_features)]\n\n# Import train set\ntrain = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl')","metadata":{"execution":{"iopub.status.busy":"2022-03-06T09:41:24.861165Z","iopub.execute_input":"2022-03-06T09:41:24.861718Z","iopub.status.idle":"2022-03-06T09:41:26.955277Z","shell.execute_reply.started":"2022-03-06T09:41:24.86168Z","shell.execute_reply":"2022-03-06T09:41:26.954352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Data Wrangling\n\nThis section handles:\n\n* Selecting the independent and dependent features from the data set.\n* Dropping time_id feature (will not be utilized in modeling).\n* Create an integer look up layer for investment _id feature.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"investment_id = train.pop(\"investment_id\")\ninvestment_id.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-06T09:41:26.956842Z","iopub.execute_input":"2022-03-06T09:41:26.957103Z","iopub.status.idle":"2022-03-06T09:41:26.980317Z","shell.execute_reply.started":"2022-03-06T09:41:26.957064Z","shell.execute_reply":"2022-03-06T09:41:26.979539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = train.pop(\"time_id\")","metadata":{"execution":{"iopub.status.busy":"2022-03-06T09:41:26.98249Z","iopub.execute_input":"2022-03-06T09:41:26.982798Z","iopub.status.idle":"2022-03-06T09:41:26.995732Z","shell.execute_reply.started":"2022-03-06T09:41:26.982763Z","shell.execute_reply":"2022-03-06T09:41:26.995108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train.pop(\"target\")\ny.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-06T09:41:26.996896Z","iopub.execute_input":"2022-03-06T09:41:26.997675Z","iopub.status.idle":"2022-03-06T09:41:27.018926Z","shell.execute_reply.started":"2022-03-06T09:41:26.997639Z","shell.execute_reply":"2022-03-06T09:41:27.017882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.1 Create an IntegerLookup layer for investment_id input","metadata":{}},{"cell_type":"code","source":"%%time\ninvestment_ids = list(investment_id.unique())\ninvestment_id_size = len(investment_ids) + 1\ninvestment_id_lookup_layer = layers.IntegerLookup(max_tokens=investment_id_size)\nwith tf.device(\"cpu\"):\n    investment_id_lookup_layer.adapt(investment_id)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T09:41:27.020009Z","iopub.execute_input":"2022-03-06T09:41:27.020705Z","iopub.status.idle":"2022-03-06T09:42:23.663444Z","shell.execute_reply.started":"2022-03-06T09:41:27.02067Z","shell.execute_reply":"2022-03-06T09:42:23.661994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Utility Functions","metadata":{}},{"cell_type":"code","source":"# Making Tesorflow dataset\nimport tensorflow as tf\ndef preprocess(X, y):\n    \n    \"\"\"\n    .Pre-processing a tensorflow dataset\n    \n    Parameters\n    ----------\n    X : array, a list of features\n\n    y : array, a feature\n    \n    \n    \"\"\"\n    return X, y\n\n\ndef make_dataset(feature, investment_id, y, batch_size=1024, mode=\"train\"):\n    \n    \"\"\" Function to create a source dataset compatable with tensorflow. \n    In addition a dataset transformation is applied  and the data is shuffled \n    if it is part of the training set. \n\n    Parameters\n    ----------\n    feature : array, shape = [n, 300]\n        300 annonymised features.\n    investment_id : list of int, shape = [n]\n        List of investment Ids.\n    y : array, shape = [n]\n        Array containing target values we wish to predict.\n    batch_size : int, default = 1024\n        Size of batches.\n    mode : string, default = \"train\"\n        Variable used to specify if the data if from the training, test or\n        validation data sets.\n    \n    Returns\n    -------\n    ds : tensorflow dataset, class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset' \n        Dataset in format compatible for training model.\n    \n    \"\"\"\n    \n    ## Read elements from memory\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature), y))\n    \n    ## Map preprocess function\n    ds = ds.map(preprocess)\n    \n    ## If mode is set to train shuffle data\n    if mode == \"train\":\n        ds = ds.shuffle(256)\n        \n    # Combine consecutive elements of this dataset into batches.\n    # Cache the elements in dataset\n    # Allow later elements to be prepared while the current element is being processed (prefetch)\n    \n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    \n    return ds","metadata":{"execution":{"iopub.status.busy":"2022-03-06T09:42:23.664765Z","iopub.execute_input":"2022-03-06T09:42:23.665494Z","iopub.status.idle":"2022-03-06T09:42:23.673275Z","shell.execute_reply.started":"2022-03-06T09:42:23.665453Z","shell.execute_reply":"2022-03-06T09:42:23.672569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Model architecture and Load Weights","metadata":{}},{"cell_type":"code","source":"# Define the three models used for the ensemble learning\n\ndef get_model():   \n    \"\"\" \n    \n    Function to define the multi-input keras model 1 architecture. \n\n    Returns\n    -------\n    \n    model : model, class 'keras.engine.functional.Functional'\n        Model groups layers into an object with training and inference features.\n    \n    \"\"\"\n    \n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n    return model\n\n\ndef get_model2():\n    \"\"\" \n    \n    Function to define the multi-input keras model 2 architecture. \n\n    Returns\n    -------\n    \n    model : model, class 'keras.engine.functional.Functional'\n        Model groups layers into an object with training and inference features.\n    \n    \"\"\"\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)    \n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n  \n   \n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dropout(0.65)(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n   # x = layers.Dropout(0.2)(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n  #  x = layers.Dropout(0.4)(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.75)(x)\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n    return model\n\n\ndef get_model3():\n    \"\"\" \n    \n    Function to define the multi-input keras model 3 architecture. \n\n    Returns\n    -------\n    \n    model : model, class 'keras.engine.functional.Functional'\n        Model groups layers into an object with training and inference features.\n    \n    \"\"\"\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float32)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dropout(0.5)(investment_id_x)\n    investment_id_x = layers.Dense(32, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dropout(0.5)(investment_id_x)\n    \n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dropout(0.5)(feature_x)\n    feature_x = layers.Dense(128, activation='swish')(feature_x)\n    feature_x = layers.Dropout(0.5)(feature_x)\n    feature_x = layers.Dense(64, activation='swish')(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    x = layers.Dropout(0.5)(x)\n    x = layers.Dense(64, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.5)(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.5)(x)\n    x = layers.Dense(16, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.5)(x)\n    output = layers.Dense(1)(x)\n    output = tf.keras.layers.BatchNormalization(axis=1)(output)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2022-03-06T09:42:23.674538Z","iopub.execute_input":"2022-03-06T09:42:23.674932Z","iopub.status.idle":"2022-03-06T09:42:23.704988Z","shell.execute_reply.started":"2022-03-06T09:42:23.674895Z","shell.execute_reply":"2022-03-06T09:42:23.704296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save memory\ndel train\ndel investment_id\ndel y\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-03-06T09:42:23.70621Z","iopub.execute_input":"2022-03-06T09:42:23.706467Z","iopub.status.idle":"2022-03-06T09:42:23.881836Z","shell.execute_reply.started":"2022-03-06T09:42:23.706431Z","shell.execute_reply":"2022-03-06T09:42:23.881073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n\n#initialize models list and load weights of the models trained outside this notebook\n\nmodels = []\nfor i in range(5):\n    model = get_model()\n    model.load_weights(f'../input/base-model-dnn/model_{i}')\n    models.append(model)\n\nfor i in range(10):\n    model = get_model2()\n    model.load_weights(f'../input/model-2-10fold/model_{i}')\n    models.append(model)\n    \n    \nfor i in range(10):\n    model = get_model3()\n    model.load_weights(f'../input/model-3-10fold/model_{i}')\n    models.append(model)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T09:42:23.884558Z","iopub.execute_input":"2022-03-06T09:42:23.884882Z","iopub.status.idle":"2022-03-06T09:42:32.098343Z","shell.execute_reply.started":"2022-03-06T09:42:23.884846Z","shell.execute_reply":"2022-03-06T09:42:32.096721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Model Ensemble","metadata":{}},{"cell_type":"code","source":"def preprocess_test(investment_id, feature):\n    return (investment_id, feature), 0\n\ndef make_test_dataset(feature, investment_id, batch_size=1024):\n    \"\"\" Function to create a source dataset from the test features compatable \n    with tensorflow. \n    In addition a dataset transformation is applied  and the data is shuffled \n    if it is part of the training set.\n\n    Parameters\n    ----------\n    feature : array, shape = [n, 300]\n        Ground truth (correct) target values.\n    investment_id : list of int, shape = [n]\n        List of investment Ids.\n    batch_size : int, default = 1024\n        Size of batches.\n    \n    Returns\n    -------\n    ds : tensorflow dataset, class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset' \n        Dataset in format compatible for training model.\n        .\n    \"\"\"\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature)))\n    ds = ds.map(preprocess_test)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\n\ndef inference(models, ds):\n    \"\"\" Make predictions unsing n models in models and return mean of predictions.\n    Parameters\n    ----------\n    models : array like, shape = [n]\n        Trained models.\n    ds : tensorflow dataset, class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset' \n        Dataset in format compatible for training model.\n    Returns\n    -------\n    mean_y_pred : float\n        Mean values of preditions made my each model in models.\n    \n    \"\"\"\n    y_preds = []\n    for model in models:\n        y_pred = model.predict(ds)\n        y_preds.append(y_pred)\n    return np.mean(y_preds, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T09:42:32.099759Z","iopub.execute_input":"2022-03-06T09:42:32.100007Z","iopub.status.idle":"2022-03-06T09:42:32.107878Z","shell.execute_reply.started":"2022-03-06T09:42:32.099973Z","shell.execute_reply":"2022-03-06T09:42:32.107025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. API Submission\n\nFinaly we call kagle's API for test data and make predictions\n","metadata":{}},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test() \nfor (test_df, sample_prediction_df) in iter_test:\n    ds = make_test_dataset(test_df[features], test_df[\"investment_id\"])\n    sample_prediction_df['target'] = inference(models, ds)\n    env.predict(sample_prediction_df) \n    display(sample_prediction_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T09:42:32.109324Z","iopub.execute_input":"2022-03-06T09:42:32.109566Z","iopub.status.idle":"2022-03-06T09:42:37.351186Z","shell.execute_reply.started":"2022-03-06T09:42:32.109534Z","shell.execute_reply":"2022-03-06T09:42:37.350389Z"},"trusted":true},"execution_count":null,"outputs":[]}]}