{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nThis notebook details model 3 architecture and training. Model 3 is an ensemble of 10 models trained on the ubiquant dataset using stratified cross validation(10 folds). There will be other ensembles of models that will be utilized in conjuction with model 1 to make the final predictions on the test set for submission.","metadata":{"papermill":{"duration":0.016475,"end_time":"2022-01-25T15:39:01.467349","exception":false,"start_time":"2022-01-25T15:39:01.450874","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"This notebook is made up of the following sections:\nâ€‹\n1. Importing libraries\n2. Data importation\n3. Data wrangling\n4. Utility functions\n5. Model architecture\n6. Model training\n7. Api Submission\n8. Evaluation\n9. Next Steps\n\nNB: This notebook is similar to model 1 and 2 notebook with difference in model architecture.","metadata":{}},{"cell_type":"markdown","source":"## 1. Importing Libraries\n","metadata":{}},{"cell_type":"code","source":"import os                                               # Functions for interacting with the OS\nimport pandas as pd                                     # Data manipulation\nimport numpy as np                                      # Mathematical functions\nimport gc                                               # Automatically releases memory when an object is no longer used\nimport matplotlib.pyplot as plt                         # Plotting\nimport tensorflow as tf                                 # Deep learning API\nfrom tensorflow.keras import layers                     # Deep learning (defining layers)\nfrom tensorflow import keras                            # Deep learning framework\nfrom scipy import stats                                 # Scientific computing and technical computing\nimport random                                           # Generate random numbers\nimport seaborn as sns                                   # Plotting\nfrom scipy.stats import *                               # Scientific computation and functions\nimport warnings                                         # Manage warning messages and outputs\nimport pickle                                           # Serializing object structures\nimport lightgbm as lgb                                  # high performance gradient boosting framework\nfrom sklearn.model_selection import train_test_split    # Splitting data into train and test set\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-05T12:22:50.425427Z","iopub.execute_input":"2022-03-05T12:22:50.426387Z","iopub.status.idle":"2022-03-05T12:22:52.256641Z","shell.execute_reply.started":"2022-03-05T12:22:50.426323Z","shell.execute_reply":"2022-03-05T12:22:52.255809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## 2. Data Importation\n\nThis competition's dataset (18.55 GB) is too large. We will use another dataset converted to utilize less memory in pickle format. The dataset is in pickle format and utilizes less memory.","metadata":{"papermill":{"duration":0.015291,"end_time":"2022-01-25T15:39:09.296817","exception":false,"start_time":"2022-01-25T15:39:09.281526","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Track time to load dataset\n!%%time\n\n# Declare number of ananonymized features\nn_features = 300\n\n# Select anonymized features\nfeatures = [f'f_{i}' for i in range(n_features)]\n\n# Import train set\ntrain = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl')","metadata":{"papermill":{"duration":16.88418,"end_time":"2022-01-25T15:39:26.19638","exception":false,"start_time":"2022-01-25T15:39:09.3122","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-05T12:22:52.258453Z","iopub.execute_input":"2022-03-05T12:22:52.258746Z","iopub.status.idle":"2022-03-05T12:22:55.165413Z","shell.execute_reply.started":"2022-03-05T12:22:52.258714Z","shell.execute_reply":"2022-03-05T12:22:55.164592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Data Wrangling\n\nThis section handles:\n\nSelecting the independent and dependent features from the data set.\nDropping time_id feature (will not be utilized in modeling).\nCreate an integer look up layer for investment _id feature.","metadata":{}},{"cell_type":"code","source":"# Dataset dimensions\ntrain.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-05T12:22:55.167522Z","iopub.execute_input":"2022-03-05T12:22:55.16793Z","iopub.status.idle":"2022-03-05T12:22:55.175514Z","shell.execute_reply.started":"2022-03-05T12:22:55.167879Z","shell.execute_reply":"2022-03-05T12:22:55.174422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select investment _id feature for processing\ninvestment_id = train.pop(\"investment_id\")\ninvestment_id.head()","metadata":{"papermill":{"duration":0.041856,"end_time":"2022-01-25T15:39:26.254473","exception":false,"start_time":"2022-01-25T15:39:26.212617","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-05T12:22:55.177112Z","iopub.execute_input":"2022-03-05T12:22:55.177465Z","iopub.status.idle":"2022-03-05T12:22:55.199187Z","shell.execute_reply.started":"2022-03-05T12:22:55.177416Z","shell.execute_reply":"2022-03-05T12:22:55.198469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop time_id feature\n_ = train.pop(\"time_id\")","metadata":{"papermill":{"duration":0.045271,"end_time":"2022-01-25T15:39:26.316843","exception":false,"start_time":"2022-01-25T15:39:26.271572","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-05T12:22:55.201714Z","iopub.execute_input":"2022-03-05T12:22:55.202142Z","iopub.status.idle":"2022-03-05T12:22:55.214445Z","shell.execute_reply.started":"2022-03-05T12:22:55.202106Z","shell.execute_reply":"2022-03-05T12:22:55.213698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select dependent / target feature\n\ny = train.pop(\"target\")\ny.head()","metadata":{"papermill":{"duration":0.04224,"end_time":"2022-01-25T15:39:26.375506","exception":false,"start_time":"2022-01-25T15:39:26.333266","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-05T12:22:55.215757Z","iopub.execute_input":"2022-03-05T12:22:55.216274Z","iopub.status.idle":"2022-03-05T12:22:55.2312Z","shell.execute_reply.started":"2022-03-05T12:22:55.216232Z","shell.execute_reply":"2022-03-05T12:22:55.230393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1 IntegerLookup Layer\n\nAn integer lookup layer is a preprocessing layer which maps integer features to contiguous ranges. It turns integer categorical values into an encoded representation that can be read by an Embedding layer or Dense layer.\n\nThis layer maps a set of arbitrary integer input tokens into indexed integer output via a table-based vocabulary lookup.\n\nThe integer lookup layer will be one of two input branches for the multi-input keras model. Having a look up layer enables the keras deep learning model to handle both categorical and numeric features.","metadata":{"papermill":{"duration":0.016633,"end_time":"2022-01-25T15:39:26.408785","exception":false,"start_time":"2022-01-25T15:39:26.392152","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Track processing time\n!%%time\n\n# Create a list of unique investment_ids\ninvestment_ids = list(investment_id.unique())\n\n# maximum tokens\ninvestment_id_size = len(investment_ids) + 1\n\n# initialize layer\ninvestment_id_lookup_layer = layers.IntegerLookup(max_tokens=investment_id_size)\n\n# Adapt layer to data (investment_ids)\ninvestment_id_lookup_layer.adapt(pd.DataFrame({\"investment_ids\":investment_ids}))","metadata":{"papermill":{"duration":4.105382,"end_time":"2022-01-25T15:39:30.530653","exception":false,"start_time":"2022-01-25T15:39:26.425271","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-05T12:22:55.23225Z","iopub.execute_input":"2022-03-05T12:22:55.232857Z","iopub.status.idle":"2022-03-05T12:22:56.287044Z","shell.execute_reply.started":"2022-03-05T12:22:55.232805Z","shell.execute_reply":"2022-03-05T12:22:56.285985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Utility Functions\n\nThis section defines utility functions to preprocess the data prior submitting to the kaggle API.","metadata":{}},{"cell_type":"code","source":"# Making Tesorflow dataset\nimport tensorflow as tf\ndef preprocess(X, y):\n    \n    \"\"\"\n    .Pre-processing a tensorflow dataset\n    \n    Parameters\n    ----------\n    X : array, a list of features\n\n    y : array, a feature\n    \n    \n    \"\"\"\n    return X, y\n\n\ndef make_dataset(feature, investment_id, y, batch_size=1024, mode=\"train\"):\n    \n    \"\"\" Function to create a source dataset compatable with tensorflow. \n    In addition a dataset transformation is applied  and the data is shuffled \n    if it is part of the training set. \n\n    Parameters\n    ----------\n    feature : array, shape = [n, 300]\n        300 annonymised features.\n    investment_id : list of int, shape = [n]\n        List of investment Ids.\n    y : array, shape = [n]\n        Array containing target values we wish to predict.\n    batch_size : int, default = 1024\n        Size of batches.\n    mode : string, default = \"train\"\n        Variable used to specify if the data if from the training, test or\n        validation data sets.\n    \n    Returns\n    -------\n    ds : tensorflow dataset, class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset' \n        Dataset in format compatible for training model.\n    \n    \"\"\"\n    \n    ## Read elements from memory\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature), y))\n    \n    ## Map preprocess function\n    ds = ds.map(preprocess)\n    \n    ## If mode is set to train shuffle data\n    if mode == \"train\":\n        ds = ds.shuffle(4096)\n        \n    # Combine consecutive elements of this dataset into batches.\n    # Cache the elements in dataset\n    # Allow later elements to be prepared while the current element is being processed (prefetch)\n    \n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    \n    return ds","metadata":{"papermill":{"duration":0.02858,"end_time":"2022-01-25T15:39:30.614302","exception":false,"start_time":"2022-01-25T15:39:30.585722","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-05T12:22:56.288765Z","iopub.execute_input":"2022-03-05T12:22:56.289575Z","iopub.status.idle":"2022-03-05T12:22:56.298028Z","shell.execute_reply.started":"2022-03-05T12:22:56.289537Z","shell.execute_reply":"2022-03-05T12:22:56.29709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Modeling\n\nThis section defines the model architecture:\n\n* Layers\n* Activation functions\n* Optimizer\n* Loss function\n* Metrics to be tracked\n\n\nThe model architecture is a multi input keras network with 2 input branches. First branch handles investment Ids (categorical feature) while the second branch will handle remaining anonymalized 300 features (numeric features).","metadata":{"papermill":{"duration":0.017564,"end_time":"2022-01-25T15:39:30.64918","exception":false,"start_time":"2022-01-25T15:39:30.631616","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### 5.1 Activation Function\n\nSwish activation function is a smooth, non-monotonic function that consistently matches or outperforms ReLU on deep networks, it is unbounded above and bounded below.\n","metadata":{}},{"cell_type":"markdown","source":"### 5.2 Optimizer\n\nAdam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments.\n\nAdam optimizer will be used with a learning rate of 0.001","metadata":{"execution":{"iopub.status.busy":"2022-03-05T12:11:51.767557Z","iopub.execute_input":"2022-03-05T12:11:51.768673Z","iopub.status.idle":"2022-03-05T12:11:51.780245Z","shell.execute_reply.started":"2022-03-05T12:11:51.768589Z","shell.execute_reply":"2022-03-05T12:11:51.778416Z"}}},{"cell_type":"markdown","source":"### 5.3 Loss Function\n\nThe model will attempt to minimize MSE (mean squared error).","metadata":{}},{"cell_type":"markdown","source":"### 5.4 Metrics\n\nThe following metrics will be tracked during training\n\n1. **Mean Squared Error (MSE)** : Average squared difference between the estimated values and the actual value.\n2. **Mean Absolute Error (MAE)** : Average of errors between paired observations. \n3. **Mean Absolute Percentage Error (MAPE)** : measures of how accurate a focus is b percentage.","metadata":{"execution":{"iopub.status.busy":"2022-03-05T12:13:32.202808Z","iopub.execute_input":"2022-03-05T12:13:32.203868Z","iopub.status.idle":"2022-03-05T12:13:32.211253Z","shell.execute_reply.started":"2022-03-05T12:13:32.203825Z","shell.execute_reply":"2022-03-05T12:13:32.210183Z"}}},{"cell_type":"code","source":"\ndef get_model():\n    \n    \"\"\" \n    \n    Function to define the multi-input keras model architecture. \n\n    Returns\n    -------\n    \n    model : model, class 'keras.engine.functional.Functional'\n        Model groups layers into an object with training and inference features.\n    \n    \"\"\"\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    # Branch 1\n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    # Turns positive integers (indexes) into dense vectors of fixed size\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x) \n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    \n    # Branch 2\n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    \n    # Takes as input a list of tensors and returns a single tensor that is the concatenation of all inputs\n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    \n    output = layers.Dense(1)(x)\n    \n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    \n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    \n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\"])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-03-05T12:22:56.299771Z","iopub.execute_input":"2022-03-05T12:22:56.300338Z","iopub.status.idle":"2022-03-05T12:22:56.316142Z","shell.execute_reply.started":"2022-03-05T12:22:56.300291Z","shell.execute_reply":"2022-03-05T12:22:56.315069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_model()\nmodel.summary()\nkeras.utils.plot_model(model,to_file=\"model3-architecture.png\", show_shapes=True)","metadata":{"papermill":{"duration":0.209912,"end_time":"2022-01-25T15:39:30.929112","exception":false,"start_time":"2022-01-25T15:39:30.7192","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-05T12:22:56.317433Z","iopub.execute_input":"2022-03-05T12:22:56.317692Z","iopub.status.idle":"2022-03-05T12:22:56.89349Z","shell.execute_reply.started":"2022-03-05T12:22:56.317635Z","shell.execute_reply":"2022-03-05T12:22:56.89237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Model Training\n\n* Statified cross validation (a resampling procedure) will be applied to train 10 folds of data resulting in 10 models.\n* Model will be trained with batchsize of 1024 (default from make datasets function).\n* Callbacks will used to avoid overfitting by early stopping with a patience of 10 epochs.\n* The model is set for 30 epochs.\n","metadata":{}},{"cell_type":"code","source":"%%time\n# Stratified is to ensure that each fold of dataset has the same proportion of observations with a given label.\n\nfrom sklearn.model_selection import StratifiedKFold\nkfold = StratifiedKFold(10, shuffle=True, random_state=42)\nmodels = []\nfor index, (train_indices, valid_indices) in enumerate(kfold.split(train, investment_id)):\n    X_train, X_val = train.iloc[train_indices], train.iloc[valid_indices]\n    investment_id_train = investment_id[train_indices]\n    y_train, y_val = y.iloc[train_indices], y.iloc[valid_indices]\n    investment_id_val = investment_id[valid_indices]\n    train_ds = make_dataset(X_train, investment_id_train, y_train)\n    valid_ds = make_dataset(X_val, investment_id_val, y_val, mode=\"valid\")\n    model = get_model()\n    checkpoint = keras.callbacks.ModelCheckpoint(f\"model_{index}\", save_best_only=True)\n    early_stop = keras.callbacks.EarlyStopping(patience=10)\n    history = model.fit(train_ds, epochs=30, validation_data=valid_ds, callbacks=[checkpoint, early_stop])\n    models.append(keras.models.load_model(f\"model_{index}\"))\n    \n    pearson_score = stats.pearsonr(model.predict(valid_ds).ravel(), y_val.values)[0]\n    print('Pearson:', pearson_score)\n    pd.DataFrame(history.history, columns=[\"mse\", \"val_mse\"]).plot()\n    plt.title(\"MSE\")\n    plt.show()\n    pd.DataFrame(history.history, columns=[\"mae\", \"val_mae\"]).plot()\n    plt.title(\"MAE\")\n    plt.show()\n    pd.DataFrame(history.history, columns=[\"rmse\", \"val_rmse\"]).plot()\n    plt.title(\"RMSE\")\n    plt.show()\n    del investment_id_train\n    del investment_id_val\n    del X_train\n    del X_val\n    del y_train\n    del y_val\n    del train_ds\n    del valid_ds\n    gc.collect()","metadata":{"papermill":{"duration":471.71946,"end_time":"2022-01-25T15:47:23.749584","exception":false,"start_time":"2022-01-25T15:39:32.030124","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-05T12:24:43.242024Z","iopub.execute_input":"2022-03-05T12:24:43.243134Z","iopub.status.idle":"2022-03-05T12:25:40.429877Z","shell.execute_reply.started":"2022-03-05T12:24:43.243091Z","shell.execute_reply":"2022-03-05T12:25:40.429024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. API Submission\n\nThis section preprocesses the test set from the API and makes predictions from the 5 models trained on the cross validated dataset. The five predictions are averaged to a single prediction by the inference function.","metadata":{}},{"cell_type":"code","source":"def preprocess_test(investment_id, feature):\n    \"\"\" Functions to pre-process test set.\n    \n    Parameters\n    ----------\n    investment_id : list of int, shape = [n]\n        List of investment Ids.\n    feature : array, shape = [n, 300]\n        300 annonymised features.\n\n    Returns\n    -------\n\n    \"\"\"\n    return (investment_id, feature), 0\n\n\n\ndef make_test_dataset(feature, investment_id, batch_size=1024):\n    \n    \"\"\" Function to create a source dataset from the test features compatable \n    with tensorflow. \n    In addition a dataset transformation is applied  and the data is shuffled \n    if it is part of the training set.\n\n    Parameters\n    ----------\n    feature : array, shape = [n, 300]\n        Ground truth (correct) target values.\n    investment_id : list of int, shape = [n]\n        List of investment Ids.\n    batch_size : int, default = 1024\n        Size of batches.\n    \n    Returns\n    -------\n    ds : tensorflow dataset, class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset' \n        Dataset in format compatible for training model.\n        .\n    \"\"\"\n    \n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature)))\n    ds = ds.map(preprocess_test)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\n\ndef inference(models, ds):\n    \n    \"\"\" Make predictions unsing n models in models and return mean of predictions.\n\n    Parameters\n    ----------\n    models : array like, shape = [n]\n        Trained models.\n    ds : tensorflow dataset, class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset' \n        Dataset in format compatible for training model.\n    \n    Returns\n    -------\n    mean_y_pred : float\n        Mean values of preditions made my each model in models.\n    \n    \"\"\"\n    \n    y_preds = []\n    for model in models:\n        y_pred = model.predict(ds)\n        y_preds.append(y_pred)\n    return np.mean(y_preds, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-03-05T12:23:57.633746Z","iopub.execute_input":"2022-03-05T12:23:57.633991Z","iopub.status.idle":"2022-03-05T12:23:57.642573Z","shell.execute_reply.started":"2022-03-05T12:23:57.63396Z","shell.execute_reply":"2022-03-05T12:23:57.641785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test() \nfor (test_df, sample_prediction_df) in iter_test:\n    ds = make_test_dataset(test_df[features], test_df[\"investment_id\"])\n    sample_prediction_df['target'] = inference(models, ds)\n    env.predict(sample_prediction_df) ","metadata":{"execution":{"iopub.status.busy":"2022-03-05T12:23:57.643454Z","iopub.execute_input":"2022-03-05T12:23:57.644106Z","iopub.status.idle":"2022-03-05T12:23:57.869369Z","shell.execute_reply.started":"2022-03-05T12:23:57.644073Z","shell.execute_reply":"2022-03-05T12:23:57.868414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8 Evaluation\n\nSubmissions are evaluated on the mean of the Pearson correlation coefficient for each time ID.\nThe 10 model ensemble resulted in a score of **0.148**","metadata":{}},{"cell_type":"markdown","source":"## 9. Next Steps\n\nThe strategy is to use this model as an ensemble with other ensembled models to improve the scores.","metadata":{"execution":{"iopub.status.busy":"2022-03-05T12:22:04.012271Z","iopub.execute_input":"2022-03-05T12:22:04.012784Z","iopub.status.idle":"2022-03-05T12:22:04.019746Z","shell.execute_reply.started":"2022-03-05T12:22:04.012736Z","shell.execute_reply":"2022-03-05T12:22:04.018587Z"}}}]}